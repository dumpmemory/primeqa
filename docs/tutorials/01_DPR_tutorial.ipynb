{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Retrieval of documents from a corpus using Neural Information Retrieval (IR)\n",
    "\n",
    "In this tutorial you'll learn how to use a popular Neural IR system called DPR [Karpukhin2020].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a Colab Environment to run this tutorial ##\n",
    "\n",
    "Make sure to \"Enable GPU Runtime\" -> make a URL with a page with screenshots on how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PrimeQA\n",
    "\n",
    "First, we need to include the required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install primeqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process your document collection here to be ready to be stored in your Neural Search Index.\n",
    "In this step we download a publicly available .csv file from a Google Drive location and save it as .tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your input document as a .tsv\n",
    "import pandas as pd\n",
    "url='https://drive.google.com/file/d/1LULJRPgN_hfuI2kG-wH4FUwXCCdDh9zh/view?usp=sharing'\n",
    "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url)\n",
    "df.to_csv('input.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Init -- Initialize your model. In PrimeQA for searching through your corpus, we use a class called SearchableCorpus.\n",
    "\n",
    "For DPR, you need to point to a question and context encoder models available via the HuggingFace model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2023-06-14 14:01:11,971\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss with AVX2 support.\"}\n",
      "{\"time\":\"2023-06-14 14:01:12,417\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss with AVX2 support.\"}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.retriever.searchable_corpus import SearchableCorpus\n",
    "collection = SearchableCorpus(model_name=\"PrimeQA/XOR-TyDi_monolingual_DPR_ctx_encoder\", \n",
    "                              query_encoder_model_name_or_path=\"PrimeQA/XOR-TyDi_monolingual_DPR_qry_encoder\", \n",
    "                              batch_size=64, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add your documents into the searchable corpus through PrimeQA's built-in pre-processor.\n",
    "\n",
    "PrimeQA has a built-in class called DocumentCollection which pre-processes input.tsv to match the following format as needed by DPR:\n",
    "\n",
    "`id \\t text \\t title_of_document`\n",
    "\n",
    "Note: since DPR is based on an encoder language model the typical sequence length is 512 max sub-word tokens. So please make sure your documents are split into text length of ~220 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:00, 42561.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:00, 65169.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "{\"time\":\"2023-06-14 14:01:53,070\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.index_simple_corpus\", \"level\": \"INFO\", \"message\": \"wrote passages_1_of_1.json.gz.records in 0 seconds\"}\n",
      "{\"time\":\"2023-06-14 14:01:53,071\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"building index, reading data from dpr_index_dir/passages_1_of_1.json.gz.records, writing to dpr_index_dir/index_1_of_1.faiss\"}\n",
      "{\"time\":\"2023-06-14 14:01:53,121\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"processed 0 passages\"}\n",
      "{\"time\":\"2023-06-14 14:01:53,124\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"calling index.add with 75 vectors\"}\n",
      "{\"time\":\"2023-06-14 14:01:53,127\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"processed 75 passages\"}\n",
      "{\"time\":\"2023-06-14 14:01:53,127\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"finished building index, writing index file to dpr_index_dir/index_1_of_1.faiss\"}\n",
      "{\"time\":\"2023-06-14 14:01:53,132\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"took 0 seconds\"}\n",
      "{\"time\":\"2023-06-14 14:01:53,133\", \"name\": \"primeqa.ir.dense.dpr_top.torch_util.hypers_base\", \"level\": \"INFO\", \"message\": \"world_rank 0 cuda_is_available True cuda_device_cnt 1 on cccxc507, CUDA_VISIBLE_DEVICES = 0\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at PrimeQA/XOR-TyDi_monolingual_DPR_ctx_encoder were not used when initializing DPRQuestionEncoder: ['ctx_encoder.bert_model.encoder.layer.8.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.6.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.2.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.10.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.query.bias', 'ctx_encoder.bert_model.embeddings.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.5.intermediate.dense.weight', 'ctx_encoder.bert_model.embeddings.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.11.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.2.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.1.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.10.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.9.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.0.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.10.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.value.bias', 'ctx_encoder.bert_model.embeddings.position_embeddings.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.11.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.4.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.11.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.8.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.1.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.11.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.6.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.4.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.6.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.0.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.4.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.4.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.2.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.9.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.8.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.value.weight', 'ctx_encoder.bert_model.embeddings.token_type_embeddings.weight', 'ctx_encoder.bert_model.encoder.layer.6.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.7.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.3.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.6.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.2.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.5.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.0.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.9.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.0.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.0.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.4.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.6.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.embeddings.word_embeddings.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.10.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.8.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.9.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.7.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.7.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.5.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.7.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.0.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.4.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.5.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.9.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.4.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.3.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.2.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.10.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.1.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.5.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.1.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.5.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.11.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.3.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.2.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.11.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.3.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.7.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.10.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.7.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.key.bias', 'ctx_encoder.bert_model.embeddings.position_ids', 'ctx_encoder.bert_model.encoder.layer.9.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.6.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.2.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.2.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.1.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.1.output.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPRQuestionEncoder were not initialized from the model checkpoint at PrimeQA/XOR-TyDi_monolingual_DPR_ctx_encoder and are newly initialized: ['bert_model.encoder.layer.1.intermediate.dense.weight', 'bert_model.encoder.layer.0.intermediate.dense.bias', 'bert_model.encoder.layer.2.intermediate.dense.bias', 'bert_model.encoder.layer.7.attention.self.query.bias', 'bert_model.encoder.layer.10.attention.output.dense.weight', 'bert_model.encoder.layer.5.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.self.value.weight', 'bert_model.encoder.layer.2.attention.self.query.weight', 'bert_model.encoder.layer.3.output.dense.bias', 'bert_model.encoder.layer.4.attention.self.value.weight', 'bert_model.encoder.layer.11.attention.self.key.bias', 'bert_model.encoder.layer.11.intermediate.dense.bias', 'bert_model.encoder.layer.7.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.2.output.dense.weight', 'bert_model.encoder.layer.5.intermediate.dense.weight', 'bert_model.encoder.layer.8.attention.self.key.bias', 'bert_model.encoder.layer.11.attention.self.query.bias', 'bert_model.encoder.layer.4.attention.self.key.weight', 'bert_model.encoder.layer.8.attention.self.query.bias', 'bert_model.encoder.layer.8.attention.output.dense.bias', 'bert_model.encoder.layer.4.attention.self.query.weight', 'bert_model.encoder.layer.1.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.self.key.bias', 'bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.self.key.bias', 'bert_model.encoder.layer.3.output.dense.weight', 'bert_model.encoder.layer.2.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.9.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.output.dense.bias', 'bert_model.encoder.layer.9.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.4.intermediate.dense.bias', 'bert_model.encoder.layer.2.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.self.value.weight', 'bert_model.encoder.layer.8.attention.output.dense.weight', 'bert_model.encoder.layer.0.attention.output.dense.weight', 'bert_model.encoder.layer.6.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.self.query.weight', 'bert_model.encoder.layer.1.attention.self.value.weight', 'bert_model.encoder.layer.1.attention.self.query.bias', 'bert_model.encoder.layer.11.output.dense.weight', 'bert_model.encoder.layer.1.attention.self.value.bias', 'bert_model.encoder.layer.4.output.LayerNorm.bias', 'bert_model.encoder.layer.2.intermediate.dense.weight', 'bert_model.encoder.layer.9.attention.self.value.weight', 'bert_model.encoder.layer.3.attention.self.value.bias', 'bert_model.encoder.layer.9.attention.self.key.bias', 'bert_model.encoder.layer.5.output.dense.bias', 'bert_model.encoder.layer.0.output.dense.bias', 'bert_model.encoder.layer.1.output.LayerNorm.bias', 'bert_model.encoder.layer.10.intermediate.dense.weight', 'bert_model.encoder.layer.3.attention.output.dense.weight', 'bert_model.embeddings.LayerNorm.weight', 'bert_model.encoder.layer.0.attention.self.query.weight', 'bert_model.encoder.layer.1.intermediate.dense.bias', 'bert_model.encoder.layer.0.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.self.value.bias', 'bert_model.encoder.layer.7.intermediate.dense.weight', 'bert_model.encoder.layer.8.output.dense.bias', 'bert_model.encoder.layer.6.attention.self.key.bias', 'bert_model.encoder.layer.8.intermediate.dense.weight', 'bert_model.encoder.layer.9.output.LayerNorm.weight', 'bert_model.encoder.layer.10.output.dense.weight', 'bert_model.encoder.layer.3.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.1.output.dense.weight', 'bert_model.encoder.layer.5.attention.output.dense.bias', 'bert_model.encoder.layer.3.output.LayerNorm.weight', 'bert_model.encoder.layer.7.output.dense.bias', 'bert_model.encoder.layer.11.intermediate.dense.weight', 'bert_model.encoder.layer.9.attention.output.dense.weight', 'bert_model.encoder.layer.11.attention.self.query.weight', 'bert_model.encoder.layer.11.attention.self.value.weight', 'bert_model.encoder.layer.11.attention.output.dense.weight', 'bert_model.encoder.layer.3.intermediate.dense.weight', 'bert_model.encoder.layer.5.output.LayerNorm.weight', 'bert_model.encoder.layer.6.output.LayerNorm.bias', 'bert_model.encoder.layer.9.attention.self.query.weight', 'bert_model.encoder.layer.5.attention.self.query.weight', 'bert_model.encoder.layer.6.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.output.LayerNorm.weight', 'bert_model.encoder.layer.1.attention.output.dense.weight', 'bert_model.encoder.layer.2.attention.output.dense.weight', 'bert_model.encoder.layer.9.attention.self.query.bias', 'bert_model.encoder.layer.4.attention.self.value.bias', 'bert_model.encoder.layer.8.attention.self.key.weight', 'bert_model.encoder.layer.5.intermediate.dense.bias', 'bert_model.encoder.layer.7.attention.output.dense.weight', 'bert_model.encoder.layer.10.attention.self.value.weight', 'bert_model.encoder.layer.2.output.LayerNorm.bias', 'bert_model.encoder.layer.11.attention.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.value.bias', 'bert_model.encoder.layer.6.attention.self.query.bias', 'bert_model.encoder.layer.11.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.self.key.weight', 'bert_model.embeddings.token_type_embeddings.weight', 'bert_model.encoder.layer.10.intermediate.dense.bias', 'bert_model.encoder.layer.1.attention.self.query.weight', 'bert_model.encoder.layer.7.attention.self.query.weight', 'bert_model.encoder.layer.8.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.6.output.dense.weight', 'bert_model.encoder.layer.6.attention.self.value.weight', 'bert_model.encoder.layer.0.output.dense.weight', 'bert_model.encoder.layer.4.attention.output.dense.weight', 'bert_model.encoder.layer.1.attention.output.dense.bias', 'bert_model.encoder.layer.1.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.self.value.bias', 'bert_model.encoder.layer.8.intermediate.dense.bias', 'bert_model.encoder.layer.2.attention.self.value.weight', 'bert_model.encoder.layer.8.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.3.attention.output.dense.bias', 'bert_model.encoder.layer.3.attention.self.value.weight', 'bert_model.encoder.layer.5.output.LayerNorm.bias', 'bert_model.encoder.layer.6.intermediate.dense.weight', 'bert_model.embeddings.position_embeddings.weight', 'bert_model.encoder.layer.6.intermediate.dense.bias', 'bert_model.encoder.layer.4.intermediate.dense.weight', 'bert_model.encoder.layer.2.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.0.output.LayerNorm.bias', 'bert_model.encoder.layer.4.output.LayerNorm.weight', 'bert_model.encoder.layer.3.intermediate.dense.bias', 'bert_model.encoder.layer.6.attention.self.value.bias', 'bert_model.encoder.layer.6.attention.output.dense.bias', 'bert_model.encoder.layer.9.output.dense.bias', 'bert_model.encoder.layer.10.attention.self.query.bias', 'bert_model.encoder.layer.11.attention.self.value.bias', 'bert_model.encoder.layer.11.output.dense.bias', 'bert_model.encoder.layer.0.attention.self.key.weight', 'bert_model.encoder.layer.4.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'bert_model.embeddings.LayerNorm.bias', 'bert_model.encoder.layer.5.attention.output.dense.weight', 'bert_model.encoder.layer.1.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.11.attention.self.key.weight', 'bert_model.encoder.layer.1.attention.self.key.weight', 'bert_model.encoder.layer.0.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.self.key.weight', 'bert_model.encoder.layer.0.attention.self.query.bias', 'bert_model.encoder.layer.10.output.LayerNorm.weight', 'bert_model.encoder.layer.1.output.dense.bias', 'bert_model.encoder.layer.0.attention.self.key.bias', 'bert_model.encoder.layer.3.attention.self.query.bias', 'bert_model.encoder.layer.7.output.LayerNorm.weight', 'bert_model.encoder.layer.9.attention.self.value.bias', 'bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.10.attention.self.key.weight', 'bert_model.encoder.layer.8.attention.self.query.weight', 'bert_model.encoder.layer.8.attention.self.value.weight', 'bert_model.encoder.layer.11.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.intermediate.dense.weight', 'bert_model.encoder.layer.7.intermediate.dense.bias', 'bert_model.encoder.layer.6.attention.self.key.weight', 'bert_model.encoder.layer.5.attention.self.key.bias', 'bert_model.encoder.layer.1.output.LayerNorm.weight', 'bert_model.encoder.layer.6.attention.output.dense.weight', 'bert_model.encoder.layer.7.attention.self.key.bias', 'bert_model.encoder.layer.2.attention.output.dense.bias', 'bert_model.encoder.layer.5.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.output.dense.bias', 'bert_model.encoder.layer.6.attention.self.query.weight', 'bert_model.encoder.layer.7.output.LayerNorm.bias', 'bert_model.encoder.layer.5.attention.self.query.bias', 'bert_model.encoder.layer.9.attention.self.key.weight', 'bert_model.encoder.layer.4.attention.self.query.bias', 'bert_model.encoder.layer.9.intermediate.dense.bias', 'bert_model.encoder.layer.8.output.LayerNorm.weight', 'bert_model.encoder.layer.9.intermediate.dense.weight', 'bert_model.encoder.layer.2.attention.self.query.bias', 'bert_model.encoder.layer.4.output.dense.weight', 'bert_model.encoder.layer.5.attention.self.key.weight', 'bert_model.encoder.layer.9.attention.output.dense.bias', 'bert_model.encoder.layer.3.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.self.value.bias', 'bert_model.encoder.layer.10.attention.self.value.bias', 'bert_model.encoder.layer.9.output.dense.weight', 'bert_model.encoder.layer.3.output.LayerNorm.bias', 'bert_model.embeddings.word_embeddings.weight', 'bert_model.encoder.layer.4.output.dense.bias', 'bert_model.encoder.layer.7.attention.self.key.weight', 'bert_model.encoder.layer.0.attention.output.dense.bias', 'bert_model.encoder.layer.9.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.self.value.bias', 'bert_model.encoder.layer.3.attention.self.query.weight', 'bert_model.encoder.layer.4.attention.output.dense.bias', 'bert_model.encoder.layer.5.output.dense.weight', 'bert_model.encoder.layer.8.output.dense.weight', 'bert_model.encoder.layer.6.output.dense.bias', 'bert_model.encoder.layer.6.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.7.output.dense.weight', 'bert_model.encoder.layer.10.output.LayerNorm.bias', 'bert_model.encoder.layer.2.output.LayerNorm.weight', 'bert_model.encoder.layer.3.attention.self.key.bias', 'bert_model.encoder.layer.2.output.dense.bias', 'bert_model.encoder.layer.10.output.dense.bias', 'bert_model.encoder.layer.8.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.self.value.weight', 'bert_model.encoder.layer.7.attention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRContextEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2023-06-14 14:01:54,550\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Using sharded faiss, reading shards from dpr_index_dir\"}\n",
      "{\"time\":\"2023-06-14 14:01:54,551\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Reading passages_1_of_1.json.gz.records\"}\n",
      "{\"time\":\"2023-06-14 14:01:54,552\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Using sharded faiss with 1 shards.\"}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.ir.util.corpus_reader import DocumentCollection\n",
    "doc_collection = DocumentCollection(\"input.tsv\")\n",
    "\n",
    "collection.add_documents(doc_collection.get_processed_collection())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Search -- start asking questions.\n",
    "\n",
    "Your queries can be a list. You can also retrieve the scores of retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"titles\": [\n",
      "            \"\\\"Akira Kurosawa\\\"\",\n",
      "            \"\\\"Alfred Nobel\\\"\",\n",
      "            \"\\\"Alkali metal\\\"\",\n",
      "            \"\\\"Alkali metal\\\"\",\n",
      "            \"Anatomy\",\n",
      "            \"Anime\",\n",
      "            \"Anatomy\",\n",
      "            \"\\\"Albert Einstein\\\"\",\n",
      "            \"\\\"Albert Einstein\\\"\",\n",
      "            \"\\\"Alkali metal\\\"\"\n",
      "        ],\n",
      "        \"texts\": [\n",
      "            \"Akira Kurosawa Akira Kurosawa (, \\\"Kurosawa Akira\\\"; March 23, 1910 \\u2013 September 6, 1998) was a Japanese film director and screenwriter, who directed 30 films in a career spanning 57 years. He is regarded as one of the most important and influential filmmakers in the history of cinema. Kurosawa entered the Japanese film industry in 1936, following a brief stint as a painter. After years of working on numerous films as an assistant director and scriptwriter, he made his debut as a director during World War II with the popular action film \\\"Sanshiro Sugata\\\" (a.k.a. \\\"Judo Saga\\\"). After the war,\",\n",
      "            \"was adopted as the standard technology for mining in the \\\"Age of Engineering\\\" bringing Nobel a great amount of financial success, though at a significant cost to his health. An offshoot of this research resulted in Nobel's invention of ballistite, the precursor of many modern smokeless powder explosives and still used as a rocket propellant. In 1888 Alfred's brother Ludvig died while visiting Cannes and a French newspaper erroneously published Alfred's obituary. It condemned him for his invention of dynamite and is said to have brought about his decision to leave a better legacy after his death. The obituary stated,\",\n",
      "            \"Lithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits. The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride. Sodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 \\u00b0C through the use of a Downs cell. Extremely pure sodium can be produced through the thermal decomposition of sodium azide. Potassium occurs in many minerals, such as sylvite (potassium chloride). Previously, potassium was generally made from the electrolysis of\",\n",
      "            \"and the Dead Sea. Despite their near-equal abundance in Earth's crust, sodium is far more common than potassium in the ocean, both because potassium's larger size makes its salts less soluble, and because potassium is bound by silicates in soil and what potassium leaches is absorbed far more readily by plant life than sodium. Despite its chemical similarity, lithium typically does not occur together with sodium or potassium due to its smaller size. Due to its relatively low reactivity, it can be found in seawater in large amounts; it is estimated that seawater is approximately 0.14 to 0.25 parts per\",\n",
      "            \"seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, along with histology (the study of tissues), and embryology (the study of an organism in its immature condition). Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted\",\n",
      "            \"Osamu Tezuka adapted and simplified many Disney animation techniques to reduce costs and to limit the number of frames in productions. He intended this as a temporary measure to allow him to produce material on a tight schedule with inexperienced animation staff. \\\"Three Tales\\\", aired in 1960, was the first anime shown on television. The first anime television series was \\\"Otogi Manga Calendar\\\", aired from 1961 to 1964. The 1970s saw a surge of growth in the popularity of \\\"manga\\\", Japanese comic books and graphic novels, many of which were later animated. The work of Osamu Tezuka drew particular attention:\",\n",
      "            \"related disciplines, and they are often studied together. Human anatomy is one of the essential basic sciences that are applied in medicine. The discipline of anatomy is divided into macroscopic and microscopic anatomy. Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells. The history of anatomy is characterized by a progressive understanding of the functions of the\",\n",
      "            \"to Einstein in 1922. Footnotes Citations Albert Einstein Albert Einstein (; ; 14 March 1879 \\u2013 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass\\u2013energy equivalence formula , which has been dubbed \\\"the world's most famous equation\\\". He received the 1921 Nobel Prize in Physics \\\"for his services to theoretical physics, and especially for his discovery of the law of\",\n",
      "            \"Albert Einstein Albert Einstein (; ; 14 March 1879 \\u2013 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass\\u2013energy equivalence formula , which has been dubbed \\\"the world's most famous equation\\\". He received the 1921 Nobel Prize in Physics \\\"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\\\", a pivotal step\",\n",
      "            \"million (ppm) or 25 micromolar. Its diagonal relationship with magnesium often allows it to replace magnesium in ferromagnesium minerals, where its crustal concentration is about 18 ppm, comparable to that of gallium and niobium. Commercially, the most important lithium mineral is spodumene, which occurs in large deposits worldwide. Rubidium is approximately as abundant as zinc and more abundant than copper. It occurs naturally in the minerals leucite, pollucite, carnallite, zinnwaldite, and lepidolite, although none of these contain only rubidium and no other alkali metals. Caesium is more abundant than some commonly known elements, such as antimony, cadmium, tin, and tungsten,\"\n",
      "        ],\n",
      "        \"scores\": [\n",
      "            20.53232192993164,\n",
      "            19.748794555664062,\n",
      "            19.331661224365234,\n",
      "            18.253440856933594,\n",
      "            16.839900970458984,\n",
      "            15.676227569580078,\n",
      "            14.630046844482422,\n",
      "            13.893537521362305,\n",
      "            13.699307441711426,\n",
      "            12.896442413330078\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"titles\": [\n",
      "            \"\\\"Alfred Nobel\\\"\",\n",
      "            \"\\\"Alkali metal\\\"\",\n",
      "            \"Anatomy\",\n",
      "            \"\\\"Akira Kurosawa\\\"\",\n",
      "            \"\\\"Alkali metal\\\"\",\n",
      "            \"Anime\",\n",
      "            \"Anatomy\",\n",
      "            \"Anatomy\",\n",
      "            \"Ashoka\",\n",
      "            \"\\\"The Ashes\\\"\"\n",
      "        ],\n",
      "        \"texts\": [\n",
      "            \"was adopted as the standard technology for mining in the \\\"Age of Engineering\\\" bringing Nobel a great amount of financial success, though at a significant cost to his health. An offshoot of this research resulted in Nobel's invention of ballistite, the precursor of many modern smokeless powder explosives and still used as a rocket propellant. In 1888 Alfred's brother Ludvig died while visiting Cannes and a French newspaper erroneously published Alfred's obituary. It condemned him for his invention of dynamite and is said to have brought about his decision to leave a better legacy after his death. The obituary stated,\",\n",
      "            \"Lithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits. The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride. Sodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 \\u00b0C through the use of a Downs cell. Extremely pure sodium can be produced through the thermal decomposition of sodium azide. Potassium occurs in many minerals, such as sylvite (potassium chloride). Previously, potassium was generally made from the electrolysis of\",\n",
      "            \"seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, along with histology (the study of tissues), and embryology (the study of an organism in its immature condition). Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted\",\n",
      "            \"Akira Kurosawa Akira Kurosawa (, \\\"Kurosawa Akira\\\"; March 23, 1910 \\u2013 September 6, 1998) was a Japanese film director and screenwriter, who directed 30 films in a career spanning 57 years. He is regarded as one of the most important and influential filmmakers in the history of cinema. Kurosawa entered the Japanese film industry in 1936, following a brief stint as a painter. After years of working on numerous films as an assistant director and scriptwriter, he made his debut as a director during World War II with the popular action film \\\"Sanshiro Sugata\\\" (a.k.a. \\\"Judo Saga\\\"). After the war,\",\n",
      "            \"and the Dead Sea. Despite their near-equal abundance in Earth's crust, sodium is far more common than potassium in the ocean, both because potassium's larger size makes its salts less soluble, and because potassium is bound by silicates in soil and what potassium leaches is absorbed far more readily by plant life than sodium. Despite its chemical similarity, lithium typically does not occur together with sodium or potassium due to its smaller size. Due to its relatively low reactivity, it can be found in seawater in large amounts; it is estimated that seawater is approximately 0.14 to 0.25 parts per\",\n",
      "            \"Osamu Tezuka adapted and simplified many Disney animation techniques to reduce costs and to limit the number of frames in productions. He intended this as a temporary measure to allow him to produce material on a tight schedule with inexperienced animation staff. \\\"Three Tales\\\", aired in 1960, was the first anime shown on television. The first anime television series was \\\"Otogi Manga Calendar\\\", aired from 1961 to 1964. The 1970s saw a surge of growth in the popularity of \\\"manga\\\", Japanese comic books and graphic novels, many of which were later animated. The work of Osamu Tezuka drew particular attention:\",\n",
      "            \"related disciplines, and they are often studied together. Human anatomy is one of the essential basic sciences that are applied in medicine. The discipline of anatomy is divided into macroscopic and microscopic anatomy. Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells. The history of anatomy is characterized by a progressive understanding of the functions of the\",\n",
      "            \"Magnetic resonance imaging, computed tomography, and ultrasound imaging have all enabled examination of internal structures in unprecedented detail to a degree far beyond the imagination of earlier generations. Anatomy Anatomy (Greek anatom\\u0113, \\\"dissection\\\") is the branch of biology concerned with the study of the structure of organisms and their parts. Anatomy is a branch of natural science which deals with the structural organization of living things. It is an old science, having its beginnings in prehistoric times. Anatomy is inherently tied to developmental biology, embryology, comparative anatomy, evolutionary biology, and phylogeny, as these are the processes by which anatomy is\",\n",
      "            \"Wheel of Dharma). The wheel has 24 spokes which represent the 12 Laws of Dependent Origination and the 12 Laws of Dependent Termination. The Ashoka Chakra has been widely inscribed on many relics of the Mauryan Emperor, most prominent among which is the Lion Capital of Sarnath and The Ashoka Pillar. The most visible use of the Ashoka Chakra today is at the centre of the National flag of the Republic of India (adopted on 22 July 1947), where it is rendered in a Navy-blue color on a White background, by replacing the symbol of Charkha (Spinning wheel) of the\",\n",
      "            \"The Ashes The Ashes is a Test cricket series played between England and Australia. The Ashes are regarded as being held by the team that most recently won the Test series. If the test series is drawn, the team that currently holds the Ashes retains the trophy. The term originated in a satirical obituary published in a British newspaper, \\\"The Sporting Times\\\", immediately after Australia's 1882 victory at The Oval, its first Test win on English soil. The obituary stated that English cricket had died, and \\\"the body will be cremated and the ashes taken to Australia\\\". The mythical ashes\"\n",
      "        ],\n",
      "        \"scores\": [\n",
      "            16.206218719482422,\n",
      "            16.09585952758789,\n",
      "            15.218173027038574,\n",
      "            14.02076530456543,\n",
      "            12.246589660644531,\n",
      "            12.24164867401123,\n",
      "            11.844680786132812,\n",
      "            11.29114818572998,\n",
      "            8.621685028076172,\n",
      "            8.50839614868164\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "queries = ['When was Idaho split in two?' , 'Who was Danny Nozel']\n",
    "retrieved_doc_ids, passages = collection.search(queries)\n",
    "#res, scores = collection.search2(queries)\n",
    "import json\n",
    "print(json.dumps(passages, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Generative QA with Retrieval Augmented Generation\n",
    "\n",
    "In this tutorial, you'll learn how to run generative question answering by connecting a retriever to a generative LLM. You'll also learn how to use prompts with a generative model to tune your answers. The system should also generate a response like \"Unanswerable\" if no evidence is found.\n",
    "\n",
    "You can plug-and-play this tutorial with most models on the HuggingFace model hub and also OpenAI LLMs. Some supported models include:\n",
    " - FLAN UL2-20B\n",
    " - FLAN T5 \n",
    " - Open AI ChatGPT (gpt-3.5-turbo)\n",
    " - InstructGPT(text-davinci-003)\n",
    " - lots more.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing PrimeQA\n",
    "First, we need to include the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install primeqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process your document collection here to be ready to be stored in your Neural Search Index.\n",
    "In this step we download a publicly available .csv file from a Google Drive location and save it as .tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your input document as a .tsv\n",
    "import pandas as pd\n",
    "url='https://drive.google.com/file/d/1LULJRPgN_hfuI2kG-wH4FUwXCCdDh9zh/view?usp=sharing'\n",
    "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url)\n",
    "df.to_csv('input.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model. In PrimeQA we use the SearchableCorpus class for searching through your corpus.\n",
    "\n",
    "For DPR, you need to point to a question and context encoder models available via the HuggingFace model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2023-06-14 15:48:06,014\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss with AVX2 support.\"}\n",
      "{\"time\":\"2023-06-14 15:48:06,527\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss with AVX2 support.\"}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.retriever.searchable_corpus import SearchableCorpus\n",
    "retriever = SearchableCorpus(model_name=\"PrimeQA/XOR-TyDi_monolingual_DPR_ctx_encoder\", \n",
    "                              query_encoder_model_name_or_path=\"PrimeQA/XOR-TyDi_monolingual_DPR_qry_encoder\", \n",
    "                              batch_size=64, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your documents into the searchable corpus through PrimeQA's built-in pre-processor.\n",
    "\n",
    "PrimeQA has a built-in class called DocumentCollection which pre-processes input.tsv to match the following format as needed by DPR:\n",
    "\n",
    "`id \\t text \\t title_of_document`\n",
    "\n",
    "Note: since DPR is based on an encoder language model the typical sequence length is 512 max sub-word tokens. So please make sure your documents are split into text length of ~220 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:00, 55004.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:00, 82673.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "{\"time\":\"2023-06-14 15:48:43,747\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.index_simple_corpus\", \"level\": \"INFO\", \"message\": \"wrote passages_1_of_1.json.gz.records in 0 seconds\"}\n",
      "{\"time\":\"2023-06-14 15:48:43,748\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"building index, reading data from dpr_index_dir/passages_1_of_1.json.gz.records, writing to dpr_index_dir/index_1_of_1.faiss\"}\n",
      "{\"time\":\"2023-06-14 15:48:43,799\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"processed 0 passages\"}\n",
      "{\"time\":\"2023-06-14 15:48:43,802\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"calling index.add with 75 vectors\"}\n",
      "{\"time\":\"2023-06-14 15:48:43,805\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"processed 75 passages\"}\n",
      "{\"time\":\"2023-06-14 15:48:43,805\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"finished building index, writing index file to dpr_index_dir/index_1_of_1.faiss\"}\n",
      "{\"time\":\"2023-06-14 15:48:43,809\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"took 0 seconds\"}\n",
      "{\"time\":\"2023-06-14 15:48:43,810\", \"name\": \"primeqa.ir.dense.dpr_top.torch_util.hypers_base\", \"level\": \"INFO\", \"message\": \"world_rank 0 cuda_is_available True cuda_device_cnt 1 on cccxc507, CUDA_VISIBLE_DEVICES = 0\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at PrimeQA/XOR-TyDi_monolingual_DPR_ctx_encoder were not used when initializing DPRQuestionEncoder: ['ctx_encoder.bert_model.encoder.layer.7.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.7.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.11.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.7.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.3.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.6.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.5.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.7.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.4.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.7.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.3.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.2.output.dense.bias', 'ctx_encoder.bert_model.embeddings.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.4.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.8.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.0.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.2.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.10.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.2.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.6.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.10.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.5.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.10.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.11.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.4.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.embeddings.token_type_embeddings.weight', 'ctx_encoder.bert_model.encoder.layer.6.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.10.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.0.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.3.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.6.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.9.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.6.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.2.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.9.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.embeddings.word_embeddings.weight', 'ctx_encoder.bert_model.encoder.layer.9.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.10.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.0.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.6.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.5.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.10.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.7.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.11.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.5.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.6.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.11.output.dense.bias', 'ctx_encoder.bert_model.embeddings.position_embeddings.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.1.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.4.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.11.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.key.weight', 'ctx_encoder.bert_model.embeddings.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.5.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.4.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.2.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.3.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.8.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.2.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.8.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.4.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.10.intermediate.dense.weight', 'ctx_encoder.bert_model.embeddings.position_ids', 'ctx_encoder.bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.8.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.1.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.1.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.4.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.6.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.3.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.4.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.6.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.1.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.0.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.9.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.9.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.8.attention.self.key.weight', 'ctx_encoder.bert_model.encoder.layer.4.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.4.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.11.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.0.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.1.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.3.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.7.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.3.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.9.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.2.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.9.attention.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.2.attention.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.9.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.4.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.5.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.value.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.8.output.LayerNorm.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.output.dense.weight', 'ctx_encoder.bert_model.encoder.layer.9.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.11.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.3.attention.output.dense.bias', 'ctx_encoder.bert_model.encoder.layer.1.attention.self.query.bias', 'ctx_encoder.bert_model.encoder.layer.6.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.10.attention.self.query.weight', 'ctx_encoder.bert_model.encoder.layer.1.intermediate.dense.bias', 'ctx_encoder.bert_model.encoder.layer.0.intermediate.dense.weight', 'ctx_encoder.bert_model.encoder.layer.7.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.key.bias', 'ctx_encoder.bert_model.encoder.layer.2.output.LayerNorm.bias', 'ctx_encoder.bert_model.encoder.layer.0.attention.self.value.bias', 'ctx_encoder.bert_model.encoder.layer.5.attention.self.key.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPRQuestionEncoder were not initialized from the model checkpoint at PrimeQA/XOR-TyDi_monolingual_DPR_ctx_encoder and are newly initialized: ['bert_model.encoder.layer.6.attention.self.query.weight', 'bert_model.encoder.layer.3.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.self.query.bias', 'bert_model.encoder.layer.1.intermediate.dense.weight', 'bert_model.encoder.layer.4.output.dense.bias', 'bert_model.encoder.layer.7.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.3.output.dense.weight', 'bert_model.encoder.layer.7.attention.self.query.bias', 'bert_model.encoder.layer.2.attention.output.dense.bias', 'bert_model.encoder.layer.10.attention.output.dense.weight', 'bert_model.encoder.layer.2.attention.self.value.bias', 'bert_model.encoder.layer.10.attention.self.value.bias', 'bert_model.encoder.layer.6.attention.self.key.bias', 'bert_model.encoder.layer.1.output.dense.bias', 'bert_model.encoder.layer.9.intermediate.dense.bias', 'bert_model.encoder.layer.1.attention.self.key.bias', 'bert_model.encoder.layer.1.attention.self.query.weight', 'bert_model.encoder.layer.7.output.dense.bias', 'bert_model.encoder.layer.10.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.self.query.bias', 'bert_model.embeddings.LayerNorm.bias', 'bert_model.encoder.layer.6.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.7.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.8.output.dense.weight', 'bert_model.encoder.layer.1.output.dense.weight', 'bert_model.encoder.layer.10.output.dense.weight', 'bert_model.encoder.layer.0.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.11.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.key.weight', 'bert_model.encoder.layer.4.attention.self.key.weight', 'bert_model.encoder.layer.5.intermediate.dense.weight', 'bert_model.encoder.layer.3.attention.self.key.weight', 'bert_model.encoder.layer.0.attention.output.dense.weight', 'bert_model.encoder.layer.6.attention.self.value.weight', 'bert_model.encoder.layer.6.attention.output.dense.weight', 'bert_model.encoder.layer.8.intermediate.dense.weight', 'bert_model.encoder.layer.11.attention.self.query.bias', 'bert_model.encoder.layer.10.attention.self.query.weight', 'bert_model.encoder.layer.0.attention.self.key.bias', 'bert_model.encoder.layer.0.attention.self.query.weight', 'bert_model.encoder.layer.4.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.self.value.bias', 'bert_model.encoder.layer.8.output.LayerNorm.bias', 'bert_model.encoder.layer.8.attention.self.query.weight', 'bert_model.encoder.layer.1.attention.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.value.weight', 'bert_model.encoder.layer.3.output.dense.bias', 'bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.6.attention.self.value.bias', 'bert_model.encoder.layer.2.attention.self.query.weight', 'bert_model.encoder.layer.6.output.dense.weight', 'bert_model.encoder.layer.1.attention.self.query.bias', 'bert_model.encoder.layer.7.attention.self.query.weight', 'bert_model.encoder.layer.3.attention.self.key.bias', 'bert_model.embeddings.token_type_embeddings.weight', 'bert_model.encoder.layer.3.attention.self.query.bias', 'bert_model.encoder.layer.3.intermediate.dense.weight', 'bert_model.encoder.layer.0.output.dense.weight', 'bert_model.encoder.layer.0.intermediate.dense.bias', 'bert_model.encoder.layer.0.output.LayerNorm.weight', 'bert_model.encoder.layer.7.attention.output.dense.bias', 'bert_model.encoder.layer.9.attention.output.dense.bias', 'bert_model.encoder.layer.4.attention.output.dense.weight', 'bert_model.encoder.layer.11.output.dense.weight', 'bert_model.encoder.layer.6.attention.self.key.weight', 'bert_model.encoder.layer.10.attention.self.query.bias', 'bert_model.encoder.layer.0.attention.self.key.weight', 'bert_model.encoder.layer.4.attention.self.query.weight', 'bert_model.encoder.layer.9.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.self.value.bias', 'bert_model.encoder.layer.3.attention.output.dense.weight', 'bert_model.encoder.layer.6.output.LayerNorm.weight', 'bert_model.encoder.layer.11.attention.self.query.weight', 'bert_model.encoder.layer.5.attention.self.value.weight', 'bert_model.embeddings.word_embeddings.weight', 'bert_model.encoder.layer.1.attention.self.key.weight', 'bert_model.encoder.layer.1.attention.self.value.weight', 'bert_model.encoder.layer.0.output.LayerNorm.bias', 'bert_model.encoder.layer.2.attention.self.key.bias', 'bert_model.encoder.layer.4.output.LayerNorm.bias', 'bert_model.encoder.layer.2.intermediate.dense.weight', 'bert_model.embeddings.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.dense.bias', 'bert_model.encoder.layer.8.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.11.attention.self.key.weight', 'bert_model.encoder.layer.5.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.1.output.LayerNorm.bias', 'bert_model.encoder.layer.9.intermediate.dense.weight', 'bert_model.encoder.layer.3.output.LayerNorm.bias', 'bert_model.encoder.layer.6.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.3.attention.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.value.weight', 'bert_model.encoder.layer.5.attention.self.query.bias', 'bert_model.encoder.layer.4.attention.self.value.weight', 'bert_model.encoder.layer.6.output.dense.bias', 'bert_model.encoder.layer.7.attention.self.key.weight', 'bert_model.encoder.layer.4.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.9.output.dense.bias', 'bert_model.encoder.layer.5.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.self.key.weight', 'bert_model.encoder.layer.9.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.1.output.LayerNorm.weight', 'bert_model.encoder.layer.11.attention.output.dense.bias', 'bert_model.encoder.layer.5.attention.self.key.weight', 'bert_model.encoder.layer.8.output.LayerNorm.weight', 'bert_model.encoder.layer.1.attention.output.dense.bias', 'bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.output.dense.bias', 'bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.9.attention.self.value.bias', 'bert_model.encoder.layer.0.output.dense.bias', 'bert_model.encoder.layer.11.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.self.query.weight', 'bert_model.encoder.layer.11.attention.self.value.weight', 'bert_model.encoder.layer.11.attention.self.value.bias', 'bert_model.encoder.layer.5.output.dense.bias', 'bert_model.encoder.layer.9.attention.self.key.weight', 'bert_model.encoder.layer.3.intermediate.dense.bias', 'bert_model.encoder.layer.11.attention.self.key.bias', 'bert_model.encoder.layer.7.intermediate.dense.weight', 'bert_model.encoder.layer.8.attention.self.key.weight', 'bert_model.encoder.layer.0.attention.self.value.weight', 'bert_model.encoder.layer.11.attention.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.key.bias', 'bert_model.encoder.layer.5.output.dense.weight', 'bert_model.encoder.layer.11.intermediate.dense.bias', 'bert_model.encoder.layer.3.attention.self.value.weight', 'bert_model.encoder.layer.10.output.LayerNorm.bias', 'bert_model.encoder.layer.4.attention.output.dense.bias', 'bert_model.encoder.layer.8.intermediate.dense.bias', 'bert_model.encoder.layer.2.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.self.value.weight', 'bert_model.encoder.layer.9.attention.self.query.weight', 'bert_model.encoder.layer.4.attention.self.query.bias', 'bert_model.encoder.layer.6.intermediate.dense.weight', 'bert_model.encoder.layer.10.intermediate.dense.bias', 'bert_model.encoder.layer.2.output.dense.bias', 'bert_model.encoder.layer.9.attention.self.value.weight', 'bert_model.encoder.layer.2.output.LayerNorm.weight', 'bert_model.encoder.layer.2.output.LayerNorm.bias', 'bert_model.encoder.layer.10.output.dense.bias', 'bert_model.encoder.layer.3.attention.self.value.bias', 'bert_model.encoder.layer.6.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.11.output.LayerNorm.weight', 'bert_model.encoder.layer.1.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.7.attention.self.key.bias', 'bert_model.encoder.layer.4.output.dense.weight', 'bert_model.encoder.layer.7.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.dense.weight', 'bert_model.encoder.layer.10.attention.self.value.weight', 'bert_model.encoder.layer.10.attention.output.dense.bias', 'bert_model.encoder.layer.4.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.9.attention.self.query.bias', 'bert_model.encoder.layer.4.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.0.attention.self.value.bias', 'bert_model.encoder.layer.9.output.dense.weight', 'bert_model.encoder.layer.1.attention.self.value.bias', 'bert_model.encoder.layer.6.attention.output.dense.bias', 'bert_model.encoder.layer.7.attention.output.dense.weight', 'bert_model.encoder.layer.5.attention.output.dense.weight', 'bert_model.encoder.layer.2.intermediate.dense.bias', 'bert_model.encoder.layer.10.attention.self.key.bias', 'bert_model.encoder.layer.3.output.LayerNorm.weight', 'bert_model.encoder.layer.5.intermediate.dense.bias', 'bert_model.encoder.layer.7.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.8.output.dense.bias', 'bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.self.query.bias', 'bert_model.encoder.layer.9.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.intermediate.dense.weight', 'bert_model.encoder.layer.1.attention.output.LayerNorm.bias', 'bert_model.embeddings.position_embeddings.weight', 'bert_model.encoder.layer.6.intermediate.dense.bias', 'bert_model.encoder.layer.9.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.output.dense.bias', 'bert_model.encoder.layer.6.attention.self.query.bias', 'bert_model.encoder.layer.9.attention.output.dense.weight', 'bert_model.encoder.layer.11.intermediate.dense.weight', 'bert_model.encoder.layer.7.intermediate.dense.bias', 'bert_model.encoder.layer.5.attention.self.query.weight', 'bert_model.encoder.layer.5.output.LayerNorm.bias', 'bert_model.encoder.layer.7.output.dense.weight', 'bert_model.encoder.layer.2.attention.output.dense.weight', 'bert_model.encoder.layer.9.output.LayerNorm.bias', 'bert_model.encoder.layer.4.intermediate.dense.weight', 'bert_model.encoder.layer.1.intermediate.dense.bias', 'bert_model.encoder.layer.2.output.dense.weight', 'bert_model.encoder.layer.10.intermediate.dense.weight', 'bert_model.encoder.layer.4.intermediate.dense.bias', 'bert_model.encoder.layer.8.attention.self.value.bias', 'bert_model.encoder.layer.5.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRContextEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2023-06-14 15:48:45,199\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Using sharded faiss, reading shards from dpr_index_dir\"}\n",
      "{\"time\":\"2023-06-14 15:48:45,200\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Reading passages_1_of_1.json.gz.records\"}\n",
      "{\"time\":\"2023-06-14 15:48:45,202\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Using sharded faiss with 1 shards.\"}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.ir.util.corpus_reader import DocumentCollection\n",
    "doc_collection = DocumentCollection(\"input.tsv\")\n",
    "\n",
    "retriever.add_documents(doc_collection.get_processed_collection())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Reader \n",
    "\n",
    "In this step you can use a generative LLM which can be prompted. This reader can be any of the generative models available in the HuggingFace model hub or OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeqa.components.reader import GenerativeReader\n",
    "\n",
    "reader = GenerativeReader(model_type='HuggingFace', model_name='google/flan-t5-xl')\n",
    "# setup an OpenAI generative reader : we support gpt-3.5-turbo and text-davinci-003\n",
    "# reader = GenerativeReader(model_type='OpenAI', model_name='gpt-3.5-turbo', api_key='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the RAG pipeline\n",
    "\n",
    "Attach a retriever to a generative LLM. You can then prompt it to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeqa.pipelines import RAG\n",
    "pipeline = RAG(retriever, reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start asking questions\n",
    "\n",
    "We \"run\" the pipeline we just created and also attach a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1552 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"0\": {\n",
      "        \"question\": \"When was Idaho split in two?\",\n",
      "        \"answers\": [\n",
      "            {\n",
      "                \"example_id\": 0,\n",
      "                \"span_answer_text\": \"SVB) techniques. SVB is a spectrally efficient transmission technique, but is limited in the number of channels it can transmit. The bandwidth of an SVB signal is twice the bandwidth of the modulating (or \\\"baseband\\\") signal, since the upper and lower sidebands around the carrier frequency each have a bandwidth as wide as the highest modulating frequency. Although the bandwidth of an AM signal is narrower than one using frequency modulation (FM), it is twice as wide as single-sideband techniques; it thus may be viewed as spectrally inefficient. Within a frequency band, only half as many transmissions (or \\\"channels\\\") can thus be accommodated. For this reason analog television employs a variant of single-sideband (known as SVB) techniques.\",\n",
      "                \"confidence_score\": 1\n",
      "            }\n",
      "        ],\n",
      "        \"passages\": [\n",
      "            \"(\\\"Ashoka Chakra\\\") from its base was placed onto the center of the National Flag of India. The capital contains four lions (Indian / Asiatic Lions), standing back to back, mounted on a short cylindrical abacus, with a frieze carrying sculptures in high relief of an elephant, a galloping horse, a bull, and a lion, separated by intervening spoked chariot-wheels over a bell-shaped lotus. Carved out of a single block of polished sandstone, the capital was believed to be crowned by a 'Wheel of Dharma' (Dharmachakra popularly known in India as the \\\"Ashoka Chakra\\\"). The Sarnath pillar bears one of the\",\n",
      "            \"pre-independence versions of the flag. The Ashoka Chakra can also been seen on the base of the Lion Capital of Ashoka which has been adopted as the National Emblem of India. The Ashoka Chakra was created by Ashoka during his reign. Chakra is a Sanskrit word which also means \\\"cycle\\\" or \\\"self-repeating process\\\". The process it signifies is the cycle of time\\u2014as in how the world changes with time. A few days before India became independent in August 1947, the specially-formed Constituent Assembly decided that the flag of India must be acceptable to all parties and communities. A flag with\",\n",
      "            \"these houses since the time of the Famine, however, the families that moved to Dooagh and their descendants, continued to use the village as a 'booley village'. This means that during the summer season, the younger members of the family, teenage boys and girls, would take the cattle to graze on the hillside and they would stay in the houses of the Deserted Village. This custom continued until the 1940s. Boolying was also carried out in other areas of Achill, including Annagh on Croaghaun mountain and in Curraun. At Ailt, Kildownet, you can see the remains of a similar deserted\",\n",
      "            \"Wheel of Dharma). The wheel has 24 spokes which represent the 12 Laws of Dependent Origination and the 12 Laws of Dependent Termination. The Ashoka Chakra has been widely inscribed on many relics of the Mauryan Emperor, most prominent among which is the Lion Capital of Sarnath and The Ashoka Pillar. The most visible use of the Ashoka Chakra today is at the centre of the National flag of the Republic of India (adopted on 22 July 1947), where it is rendered in a Navy-blue color on a White background, by replacing the symbol of Charkha (Spinning wheel) of the\",\n",
      "            \"for outdoor adventure activities, like surfing, kite-surfing and sea kayaking. Fishing and watersports are also popular. Sailing regattas featuring a local vessel type, the Achill Yawl, have been popular since the 19th century, though most present-day yawls, unlike their traditional working boat ancestors, have been structurally modified to promote greater speed under sail. The island's waters and underwater sites are occasionally visited by scuba divers, though Achill's unpredictable weather generally has precluded a commercially successful recreational diving industry. In 2011, the population was 2,569. The island's population has declined from around 6,000 before the Great Hunger. The table below reports\",\n",
      "            \"Lithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits. The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride. Sodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 \\u00b0C through the use of a Downs cell. Extremely pure sodium can be produced through the thermal decomposition of sodium azide. Potassium occurs in many minerals, such as sylvite (potassium chloride). Previously, potassium was generally made from the electrolysis of\",\n",
      "            \"million (ppm) or 25 micromolar. Its diagonal relationship with magnesium often allows it to replace magnesium in ferromagnesium minerals, where its crustal concentration is about 18 ppm, comparable to that of gallium and niobium. Commercially, the most important lithium mineral is spodumene, which occurs in large deposits worldwide. Rubidium is approximately as abundant as zinc and more abundant than copper. It occurs naturally in the minerals leucite, pollucite, carnallite, zinnwaldite, and lepidolite, although none of these contain only rubidium and no other alkali metals. Caesium is more abundant than some commonly known elements, such as antimony, cadmium, tin, and tungsten,\",\n",
      "            \"of lithium in humans has yet to be identified. Sodium and potassium occur in all known biological systems, generally functioning as electrolytes inside and outside cells. Sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride (also known as common salt) is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods. The Dietary Reference Intake for sodium is 1.5 grams per day, but\",\n",
      "            \"The national animal of Azerbaijan is the Karabakh horse, a mountain-steppe racing and riding horse endemic to Azerbaijan. The Karabakh horse has a reputation for its good temper, speed, elegance and intelligence. It is one of the oldest breeds, with ancestry dating to the ancient world. However, today the horse is an endangered species. Azerbaijan's flora consists of more than 4,500 species of higher plants. Due to the unique climate in Azerbaijan, the flora is much richer in the number of species than the flora of the other republics of the South Caucasus. About 67 percent of the species growing\",\n",
      "            \"AM transmission (refer to Figure 2, but only considering positive frequencies) is twice the bandwidth of the modulating (or \\\"baseband\\\") signal, since the upper and lower sidebands around the carrier frequency each have a bandwidth as wide as the highest modulating frequency. Although the bandwidth of an AM signal is narrower than one using frequency modulation (FM), it is twice as wide as single-sideband techniques; it thus may be viewed as spectrally inefficient. Within a frequency band, only half as many transmissions (or \\\"channels\\\") can thus be accommodated. For this reason analog television employs a variant of single-sideband (known as\"\n",
      "        ]\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"question\": \"Who was Danny Nozel\",\n",
      "        \"answers\": [\n",
      "            {\n",
      "                \"example_id\": 1,\n",
      "                \"span_answer_text\": \"Danny Nozel was an American comedian and actor. He was born in New York City and grew up in the Bronx. He was a member of the comedy troupe, \\\"The Danny Nozel Show\\\" and appeared on the television show \\\"The Danny Nozel Show\\\" on NBC. He also appeared on the television show \\\"The Danny Nozel Show\\\" on Fox. He was married to actress and singer, Diane Nozel. They had two children, a son, and a daughter.\",\n",
      "                \"confidence_score\": 1\n",
      "            }\n",
      "        ],\n",
      "        \"passages\": [\n",
      "            \"(\\\"Ashoka Chakra\\\") from its base was placed onto the center of the National Flag of India. The capital contains four lions (Indian / Asiatic Lions), standing back to back, mounted on a short cylindrical abacus, with a frieze carrying sculptures in high relief of an elephant, a galloping horse, a bull, and a lion, separated by intervening spoked chariot-wheels over a bell-shaped lotus. Carved out of a single block of polished sandstone, the capital was believed to be crowned by a 'Wheel of Dharma' (Dharmachakra popularly known in India as the \\\"Ashoka Chakra\\\"). The Sarnath pillar bears one of the\",\n",
      "            \"pre-independence versions of the flag. The Ashoka Chakra can also been seen on the base of the Lion Capital of Ashoka which has been adopted as the National Emblem of India. The Ashoka Chakra was created by Ashoka during his reign. Chakra is a Sanskrit word which also means \\\"cycle\\\" or \\\"self-repeating process\\\". The process it signifies is the cycle of time\\u2014as in how the world changes with time. A few days before India became independent in August 1947, the specially-formed Constituent Assembly decided that the flag of India must be acceptable to all parties and communities. A flag with\",\n",
      "            \"these houses since the time of the Famine, however, the families that moved to Dooagh and their descendants, continued to use the village as a 'booley village'. This means that during the summer season, the younger members of the family, teenage boys and girls, would take the cattle to graze on the hillside and they would stay in the houses of the Deserted Village. This custom continued until the 1940s. Boolying was also carried out in other areas of Achill, including Annagh on Croaghaun mountain and in Curraun. At Ailt, Kildownet, you can see the remains of a similar deserted\",\n",
      "            \"for outdoor adventure activities, like surfing, kite-surfing and sea kayaking. Fishing and watersports are also popular. Sailing regattas featuring a local vessel type, the Achill Yawl, have been popular since the 19th century, though most present-day yawls, unlike their traditional working boat ancestors, have been structurally modified to promote greater speed under sail. The island's waters and underwater sites are occasionally visited by scuba divers, though Achill's unpredictable weather generally has precluded a commercially successful recreational diving industry. In 2011, the population was 2,569. The island's population has declined from around 6,000 before the Great Hunger. The table below reports\",\n",
      "            \"The national animal of Azerbaijan is the Karabakh horse, a mountain-steppe racing and riding horse endemic to Azerbaijan. The Karabakh horse has a reputation for its good temper, speed, elegance and intelligence. It is one of the oldest breeds, with ancestry dating to the ancient world. However, today the horse is an endangered species. Azerbaijan's flora consists of more than 4,500 species of higher plants. Due to the unique climate in Azerbaijan, the flora is much richer in the number of species than the flora of the other republics of the South Caucasus. About 67 percent of the species growing\",\n",
      "            \"Lithium salts have to be extracted from the water of mineral springs, brine pools, and brine deposits. The metal is produced electrolytically from a mixture of fused lithium chloride and potassium chloride. Sodium occurs mostly in seawater and dried seabed, but is now produced through electrolysis of sodium chloride by lowering the melting point of the substance to below 700 \\u00b0C through the use of a Downs cell. Extremely pure sodium can be produced through the thermal decomposition of sodium azide. Potassium occurs in many minerals, such as sylvite (potassium chloride). Previously, potassium was generally made from the electrolysis of\",\n",
      "            \"Wheel of Dharma). The wheel has 24 spokes which represent the 12 Laws of Dependent Origination and the 12 Laws of Dependent Termination. The Ashoka Chakra has been widely inscribed on many relics of the Mauryan Emperor, most prominent among which is the Lion Capital of Sarnath and The Ashoka Pillar. The most visible use of the Ashoka Chakra today is at the centre of the National flag of the Republic of India (adopted on 22 July 1947), where it is rendered in a Navy-blue color on a White background, by replacing the symbol of Charkha (Spinning wheel) of the\",\n",
      "            \"a letter to a local Aspen newspaper informing them that, on Christmas Eve, he was going to use napalm to burn a number of dogs and hopefully any humans they find. The letter protests against the burning of Vietnamese people occurring overseas. The 2012 film \\\"Butcher Boys,\\\" written by Kim Henkel, is said to be loosely based on Jonathan Swift's \\\"A Modest Proposal.\\\" The film's opening scene takes place in a restaurant named \\\"J. Swift's\\\". On November 30, 2017, Jonathan Swift's 350th birthday, The Washington Post published a column entitled 'Why Alabamians should consider eating Democrats' babies\\\", by the humorous\",\n",
      "            \"of lithium in humans has yet to be identified. Sodium and potassium occur in all known biological systems, generally functioning as electrolytes inside and outside cells. Sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH; the minimum physiological requirement for sodium is 500 milligrams per day. Sodium chloride (also known as common salt) is the principal source of sodium in the diet, and is used as seasoning and preservative, such as for pickling and jerky; most of it comes from processed foods. The Dietary Reference Intake for sodium is 1.5 grams per day, but\",\n",
      "            \"in such organs as sea anemone tentacles and the body wall of sea cucumbers. Skeletal muscle contracts rapidly but has a limited range of extension. It is found in the movement of appendages and jaws. Obliquely striated muscle is intermediate between the other two. The filaments are staggered and this is the type of muscle found in earthworms that can extend slowly or make rapid contractions. In higher animals striated muscles occur in bundles attached to bone to provide movement and are often arranged in antagonistic sets. Smooth muscle is found in the walls of the uterus, bladder, intestines, stomach,\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "questions = ['When was Idaho split in two?' , 'Who was Danny Nozel']\n",
    "prompt_prefix = \"Answer the following question after looking at the text.\"\n",
    "#answers=reader.predict(questions=questions,prefix=prompt_prefix)\n",
    "answers = pipeline.run(questions, prefix=prompt_prefix)\n",
    "print(json.dumps(answers, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5d039775dffd761dc362a240b88aab365529f2df8e87d6e6e9eecd3e8d89fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Generative QA with Retrieval Augmented Generation\n",
    "\n",
    "In this tutorial, you'll learn how to run generative question answering by connecting a retriever to a generative LLM. You'll also learn how to use prompts with a generative model to tune your answers. The system should also generate a response like \"Unanswerable\" if no evidence is found.\n",
    "\n",
    "You can plug-and-play this tutorial with most models on the HuggingFace model hub and also OpenAI LLMs. Some supported models include:\n",
    " - FLAN UL2-20B\n",
    " - FLAN T5 \n",
    " - Open AI ChatGPT (gpt-3.5-turbo)\n",
    " - InstructGPT(text-davinci-003)\n",
    " - lots more.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing PrimeQA\n",
    "First, we need to include the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install primeqa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Retriever"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process your document collection here to be ready to be stored in your Neural Search Index.\n",
    "In this step we download a publicly available .csv file from a Google Drive location and save it as .tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T02:45:01.744096500Z",
     "start_time": "2023-06-18T02:45:00.654342900Z"
    }
   },
   "outputs": [],
   "source": [
    "# save your input document as a .tsv\n",
    "import pandas as pd\n",
    "url='https://drive.google.com/file/d/1LULJRPgN_hfuI2kG-wH4FUwXCCdDh9zh/view?usp=sharing'\n",
    "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url)\n",
    "df.to_csv('input.tsv', sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model. In PrimeQA we use the SearchableCorpus class for searching through your corpus.\n",
    "\n",
    "For DPR, you need to point to a question and context encoder models available via the HuggingFace model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T02:53:51.058138100Z",
     "start_time": "2023-06-18T02:53:49.591435600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2023-06-17 22:53:50,387\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss with AVX2 support.\"}\n",
      "{\"time\":\"2023-06-17 22:53:50,396\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss with AVX2 support.\"}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.util import SearchableCorpus\n",
    "retriever = SearchableCorpus(context_encoder_name_or_path=\"PrimeQA/XOR-TyDi_monolingual_DPR_ctx_encoder\",\n",
    "                             query_encoder_name_or_path=\"PrimeQA/XOR-TyDi_monolingual_DPR_qry_encoder\",\n",
    "                             batch_size=64, top_k=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your documents into the searchable corpus through PrimeQA's built-in pre-processor.\n",
    "\n",
    "PrimeQA has a built-in class called DocumentCollection which pre-processes input.tsv to match the following format as needed by DPR:\n",
    "\n",
    "`id \\t text \\t title_of_document`\n",
    "\n",
    "Note: since DPR is based on an encoder language model the typical sequence length is 512 max sub-word tokens. So please make sure your documents are split into text length of ~220 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T02:53:59.452981800Z",
     "start_time": "2023-06-18T02:53:56.506429300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2023-06-17 22:53:58,797\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.index_simple_corpus\", \"level\": \"INFO\", \"message\": \"wrote passages_1_of_1.json.gz.records in 0 seconds\"}\n",
      "{\"time\":\"2023-06-17 22:53:58,798\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"building index, reading data from dpr_index_dir/passages_1_of_1.json.gz.records, writing to dpr_index_dir/index_1_of_1.faiss\"}\n",
      "{\"time\":\"2023-06-17 22:53:58,798\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"processed 0 passages\"}\n",
      "{\"time\":\"2023-06-17 22:53:58,801\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"calling index.add with 76 vectors\"}\n",
      "{\"time\":\"2023-06-17 22:53:58,802\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"processed 76 passages\"}\n",
      "{\"time\":\"2023-06-17 22:53:58,802\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"finished building index, writing index file to dpr_index_dir/index_1_of_1.faiss\"}\n",
      "{\"time\":\"2023-06-17 22:53:58,803\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.faiss_index\", \"level\": \"INFO\", \"message\": \"took 0 seconds\"}\n",
      "{\"time\":\"2023-06-17 22:53:58,803\", \"name\": \"primeqa.ir.dense.dpr_top.torch_util.hypers_base\", \"level\": \"INFO\", \"message\": \"world_rank 0 cuda_is_available True cuda_device_cnt 1 on Radu-BigWindows, CUDA_VISIBLE_DEVICES = NOT SET\"}\n",
      "{\"time\":\"2023-06-17 22:53:59,408\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Using sharded faiss, reading shards from dpr_index_dir\"}\n",
      "{\"time\":\"2023-06-17 22:53:59,409\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Reading passages_1_of_1.json.gz.records\"}\n",
      "{\"time\":\"2023-06-17 22:53:59,410\", \"name\": \"primeqa.ir.dense.dpr_top.dpr.searcher\", \"level\": \"INFO\", \"message\": \"Using sharded faiss with 1 shards.\"}\n"
     ]
    }
   ],
   "source": [
    "retriever.add_documents(\"input.tsv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Reader \n",
    "\n",
    "In this step you can use a generative LLM which can be prompted. This reader can be any of the generative models available in the HuggingFace model hub or OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T02:54:29.529799900Z",
     "start_time": "2023-06-18T02:54:14.208300200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8170da0a721f4d55a5ded38f579d3b8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc61477931484a06a2e5e6f35601f777"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f404daf88ca641ce89ed4c130f495c71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01b7127d63e843a3aaf69f13577d1297"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4224ddb53f7f44099340bcfc15ccdfef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from primeqa.components.reader import GenerativeReader\n",
    "\n",
    "reader = GenerativeReader(model_type='HuggingFace', model_name='google/flan-t5-small')\n",
    "# setup an OpenAI generative reader : we support gpt-3.5-turbo and text-davinci-003\n",
    "# reader = GenerativeReader(model_type='OpenAI', model_name='text-davinci-003', api_key='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the RAG pipeline\n",
    "\n",
    "Attach a retriever to a generative LLM. You can then prompt it to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T02:54:36.389972400Z",
     "start_time": "2023-06-18T02:54:36.385974100Z"
    }
   },
   "outputs": [],
   "source": [
    "from primeqa.pipelines import RAG\n",
    "pipeline = RAG(retriever, reader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start asking questions\n",
    "\n",
    "We \"run\" the pipeline we just created and also attach a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T02:54:44.342134800Z",
     "start_time": "2023-06-18T02:54:40.653234800Z"
    }
   },
   "outputs": [],
   "source": [
    "questions = ['When was Idaho split in two?' , 'Who was Danny Nozel']\n",
    "prompt_prefix = \"Answer the following question after looking at the text.\"\n",
    "\n",
    "answers = pipeline.run(questions, prefix=prompt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T02:54:46.576816800Z",
     "start_time": "2023-06-18T02:54:46.570817600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>passages</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>When was Idaho split in two?</td>\n      <td>1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03-29 ; 1898-2000-03</td>\n      <td>[Passage: \"American Revolutionary War\"..., Passage: \"American Civil War\"..., Passage: \"American Revolutionary War\"...]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Who was Danny Nozel</td>\n      <td>a sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war sailor of the american civil war s</td>\n      <td>[Passage: Alaska..., Passage: \"Alexander Graham Bell\"..., Passage: \"Alkali metal\"...]</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "output = pd.DataFrame.from_records(answers)\n",
    "display(HTML(output.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5d039775dffd761dc362a240b88aab365529f2df8e87d6e6e9eecd3e8d89fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
